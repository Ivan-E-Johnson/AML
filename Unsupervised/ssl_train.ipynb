{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceab8707",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c35c3",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning Using 3D CT Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc8d67",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "[TCIA Covid 19](https://wiki.cancerimagingarchive.net/display/Public/CT+Images+in+COVID-19) dataset needs to be downloaded for the execution of this notebook, both datasets need to be downloaded. Downloading data to the `data_root` path defined variable is optimal, if data is downloaded at another location please change the path `data_root` accordingly.\n",
    "\n",
    "Please see the Readme for more details about the dataset. Data splits have already been provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707541a2",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "4dc0237b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:35:05.746199Z",
     "start_time": "2024-04-15T20:34:49.960770Z"
    }
   },
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[pillow, tqdm]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "49070e05",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "cf64bf41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:35:08.180772Z",
     "start_time": "2024-04-15T20:35:05.747585Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn import L1Loss\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.networks.nets import ViTAutoEnc\n",
    "from monai.losses import ContrastiveLoss\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.config import print_config\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    CopyItemsd,\n",
    "    SpatialPadd,\n",
    "    EnsureChannelFirstd,\n",
    "    Spacingd,\n",
    "    OneOf,\n",
    "    ScaleIntensityRanged,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCoarseDropoutd,\n",
    "    RandCoarseShuffled,\n",
    ")\n",
    "\n",
    "print_config()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.2.0\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.2.0\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: c33f1ba588ee00229a309000e888f9817b4f1934\n",
      "MONAI __file__: /Users/iejohnson/.venv/AML_Project_Supervised/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.13\n",
      "ITK version: 5.3.0\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: 2.16.2\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.66.2\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.8\n",
      "pandas version: 2.2.0\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "72e2e12c",
   "metadata": {},
   "source": [
    "##### Define file paths & output directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657217e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.normpath(\"./datalists/tcia/dataset_split.json\")\n",
    "data_root = os.path.normpath(\"/workspace/datasets/tcia/\")\n",
    "logdir_path = os.path.normpath(\"./ssl_train_logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf9d64",
   "metadata": {},
   "source": [
    "##### Create result logging directories, manage data paths & set determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(logdir_path) is False:\n",
    "    os.mkdir(logdir_path)\n",
    "\n",
    "with open(json_path, \"r\") as json_f:\n",
    "    json_data = json.load(json_f)\n",
    "\n",
    "train_data = json_data[\"training\"]\n",
    "val_data = json_data[\"validation\"]\n",
    "\n",
    "for idx, _each_d in enumerate(train_data):\n",
    "    train_data[idx][\"image\"] = os.path.join(data_root, train_data[idx][\"image\"])\n",
    "\n",
    "for idx, _each_d in enumerate(val_data):\n",
    "    val_data[idx][\"image\"] = os.path.join(data_root, val_data[idx][\"image\"])\n",
    "\n",
    "print(\"Total Number of Training Data Samples: {}\".format(len(train_data)))\n",
    "print(train_data)\n",
    "print(\"#\" * 10)\n",
    "print(\"Total Number of Validation Data Samples: {}\".format(len(val_data)))\n",
    "print(val_data)\n",
    "print(\"#\" * 10)\n",
    "\n",
    "# Set Determinism\n",
    "set_determinism(seed=123)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:40:01.735550Z",
     "start_time": "2024-04-15T20:40:01.729957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _create_image_dict(base_data_path: Path, is_testing: bool = False) -> list:\n",
    "    data_dicts = []\n",
    "    for dir in base_data_path.iterdir():\n",
    "        if dir.is_dir():\n",
    "            data_dicts.append({\"image\": dir / f\"{dir.name}_pp_t2w.nii.gz\"})\n",
    "    if is_testing:\n",
    "        data_dicts = data_dicts[:10]\n",
    "    return data_dicts\n",
    "base_data_path = Path(\"/Users/iejohnson/School/spring_2024/AML/Supervised_learning/DATA/ALL_PROSTATEx/WITHOUT_SEGMENTATION/PreProcessed\")\n",
    "print(f\"Base data path: {base_data_path}\")\n",
    "data_dicts = _create_image_dict(base_data_path, is_testing=True)"
   ],
   "id": "c1663fe8754bbfd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base data path: /Users/iejohnson/School/spring_2024/AML/Supervised_learning/DATA/ALL_PROSTATEx/WITHOUT_SEGMENTATION/PreProcessed\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "d106d4ea",
   "metadata": {},
   "source": [
    "##### Define MONAI Transforms "
   ]
  },
  {
   "cell_type": "code",
   "id": "f8ebbdd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:40:42.641969Z",
     "start_time": "2024-04-15T20:40:42.496038Z"
    }
   },
   "source": [
    "from monai.transforms import DataStatsd\n",
    "\n",
    "# Define Training Transforms\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(2.0, 2.0, 2.0), mode=(\"bilinear\")),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "        DataStatsd(keys=[\"image\"]),\n",
    "        SpatialPadd(keys=[\"image\"], spatial_size=(96, 96, 96)),\n",
    "        DataStatsd(keys=[\"image\"]),\n",
    "        RandSpatialCropSamplesd(keys=[\"image\"], roi_size=(96, 96, 96), random_size=False, num_samples=2),\n",
    "        CopyItemsd(keys=[\"image\"], times=2, names=[\"gt_image\", \"image_2\"], allow_missing_keys=False),\n",
    "        OneOf(\n",
    "            transforms=[\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image\"], prob=1.0, holes=6, spatial_size=5, dropout_holes=True, max_spatial_size=32\n",
    "                ),\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image\"], prob=1.0, holes=6, spatial_size=20, dropout_holes=False, max_spatial_size=64\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        RandCoarseShuffled(keys=[\"image\"], prob=0.8, holes=10, spatial_size=8),\n",
    "        # Please note that that if image, image_2 are called via the same transform call because of the determinism\n",
    "        # they will get augmented the exact same way which is not the required case here, hence two calls are made\n",
    "        OneOf(\n",
    "            transforms=[\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image_2\"], prob=1.0, holes=6, spatial_size=5, dropout_holes=True, max_spatial_size=32\n",
    "                ),\n",
    "                RandCoarseDropoutd(\n",
    "                    keys=[\"image_2\"], prob=1.0, holes=6, spatial_size=20, dropout_holes=False, max_spatial_size=64\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        RandCoarseShuffled(keys=[\"image_2\"], prob=0.8, holes=10, spatial_size=8),\n",
    "        DataStatsd(keys=[\"image\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "check_ds = Dataset(data=data_dicts, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image = check_data[\"image\"][0][0]\n",
    "print(f\"image shape: {image.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data statistics:\n",
      "Type: <class 'monai.data.meta_tensor.MetaTensor'> torch.float32\n",
      "Shape: torch.Size([1, 61, 61, 34])\n",
      "Value range: (0.2504201829433441, 0.27640363574028015)\n",
      "Data statistics:\n",
      "Type: <class 'monai.data.meta_tensor.MetaTensor'> torch.float32\n",
      "Shape: torch.Size([1, 96, 96, 96])\n",
      "Value range: (0.0, 0.27640363574028015)\n",
      "Data statistics:\n",
      "Type: <class 'monai.data.meta_tensor.MetaTensor'> torch.float32\n",
      "Shape: torch.Size([1, 96, 96, 96])\n",
      "Value range: (0.0, 0.2763994634151459)\n",
      "Data statistics:\n",
      "Type: <class 'monai.data.meta_tensor.MetaTensor'> torch.float32\n",
      "Shape: torch.Size([1, 96, 96, 96])\n",
      "Value range: (0.0, 0.2763994634151459)\n",
      "image shape: torch.Size([96, 96, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "4e3453c4",
   "metadata": {},
   "source": [
    "##### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "ceb5728e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:41:04.675073Z",
     "start_time": "2024-04-15T20:41:04.430616Z"
    }
   },
   "source": [
    "# Training Config\n",
    "\n",
    "# Define Network ViT backbone & Loss & Optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = ViTAutoEnc(\n",
    "    in_channels=1,\n",
    "    img_size=(96, 96, 96),\n",
    "    patch_size=(16, 16, 16),\n",
    "    pos_embed=\"conv\",\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Hyper-paramters for training loop\n",
    "max_epochs = 500\n",
    "val_interval = 2\n",
    "batch_size = 4\n",
    "lr = 1e-4\n",
    "epoch_loss_values = []\n",
    "step_loss_values = []\n",
    "epoch_cl_loss_values = []\n",
    "epoch_recon_loss_values = []\n",
    "val_loss_values = []\n",
    "best_val_loss = 1000.0\n",
    "\n",
    "recon_loss = L1Loss()\n",
    "contrastive_loss = ContrastiveLoss(temperature=0.05)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Define DataLoader using MONAI, CacheDataset needs to be used\n",
    "train_ds = Dataset(data=train_data, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = Dataset(data=val_data, transform=train_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
   ],
   "outputs": [
    {
     "ename": "OptionalImportError",
     "evalue": "from einops.layers.torch import Rearrange (No module named 'einops').\n\nFor details about installing the optional dependencies, please visit:\n    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOptionalImportError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Training Config\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Define Network ViT backbone & Loss & Optimizer\u001B[39;00m\n\u001B[1;32m      4\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mViTAutoEnc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimg_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m96\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m96\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m96\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpos_embed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m768\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmlp_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3072\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Define Hyper-paramters for training loop\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/AML_Project_Supervised/lib/python3.10/site-packages/monai/networks/nets/vitautoenc.py:96\u001B[0m, in \u001B[0;36mViTAutoEnc.__init__\u001B[0;34m(self, in_channels, img_size, patch_size, out_channels, deconv_chns, hidden_size, mlp_dim, num_layers, num_heads, pos_embed, dropout_rate, spatial_dims, qkv_bias, save_attn)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspatial_dims \u001B[38;5;241m=\u001B[39m spatial_dims\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_embedding \u001B[38;5;241m=\u001B[39m PatchEmbeddingBlock(\n\u001B[1;32m     86\u001B[0m     in_channels\u001B[38;5;241m=\u001B[39min_channels,\n\u001B[1;32m     87\u001B[0m     img_size\u001B[38;5;241m=\u001B[39mimg_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     93\u001B[0m     spatial_dims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspatial_dims,\n\u001B[1;32m     94\u001B[0m )\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList(\n\u001B[0;32m---> 96\u001B[0m     [\n\u001B[1;32m     97\u001B[0m         TransformerBlock(hidden_size, mlp_dim, num_heads, dropout_rate, qkv_bias, save_attn)\n\u001B[1;32m     98\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_layers)\n\u001B[1;32m     99\u001B[0m     ]\n\u001B[1;32m    100\u001B[0m )\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_size)\n\u001B[1;32m    103\u001B[0m new_patch_size \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m4\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspatial_dims\n",
      "File \u001B[0;32m~/.venv/AML_Project_Supervised/lib/python3.10/site-packages/monai/networks/nets/vitautoenc.py:97\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspatial_dims \u001B[38;5;241m=\u001B[39m spatial_dims\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_embedding \u001B[38;5;241m=\u001B[39m PatchEmbeddingBlock(\n\u001B[1;32m     86\u001B[0m     in_channels\u001B[38;5;241m=\u001B[39min_channels,\n\u001B[1;32m     87\u001B[0m     img_size\u001B[38;5;241m=\u001B[39mimg_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     93\u001B[0m     spatial_dims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspatial_dims,\n\u001B[1;32m     94\u001B[0m )\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList(\n\u001B[1;32m     96\u001B[0m     [\n\u001B[0;32m---> 97\u001B[0m         \u001B[43mTransformerBlock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmlp_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqkv_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_attn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_layers)\n\u001B[1;32m     99\u001B[0m     ]\n\u001B[1;32m    100\u001B[0m )\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_size)\n\u001B[1;32m    103\u001B[0m new_patch_size \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m4\u001B[39m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspatial_dims\n",
      "File \u001B[0;32m~/.venv/AML_Project_Supervised/lib/python3.10/site-packages/monai/networks/blocks/transformerblock.py:56\u001B[0m, in \u001B[0;36mTransformerBlock.__init__\u001B[0;34m(self, hidden_size, mlp_dim, num_heads, dropout_rate, qkv_bias, save_attn)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp \u001B[38;5;241m=\u001B[39m MLPBlock(hidden_size, mlp_dim, dropout_rate)\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_size)\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn \u001B[38;5;241m=\u001B[39m \u001B[43mSABlock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqkv_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_attn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_size)\n",
      "File \u001B[0;32m~/.venv/AML_Project_Supervised/lib/python3.10/site-packages/monai/networks/blocks/selfattention.py:57\u001B[0m, in \u001B[0;36mSABlock.__init__\u001B[0;34m(self, hidden_size, num_heads, dropout_rate, qkv_bias, save_attn)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_proj \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(hidden_size, hidden_size)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqkv \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(hidden_size, hidden_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m3\u001B[39m, bias\u001B[38;5;241m=\u001B[39mqkv_bias)\n\u001B[0;32m---> 57\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_rearrange \u001B[38;5;241m=\u001B[39m \u001B[43mRearrange\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mb h (qkv l d) -> qkv b l h d\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mqkv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_rearrange \u001B[38;5;241m=\u001B[39m Rearrange(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb h l d -> b l (h d)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_output \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDropout(dropout_rate)\n",
      "File \u001B[0;32m~/.venv/AML_Project_Supervised/lib/python3.10/site-packages/monai/utils/module.py:443\u001B[0m, in \u001B[0;36moptional_import.<locals>._LazyRaise.__call__\u001B[0;34m(self, *_args, **_kwargs)\u001B[0m\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_kwargs):\n\u001B[1;32m    439\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    440\u001B[0m \u001B[38;5;124;03m    Raises:\u001B[39;00m\n\u001B[1;32m    441\u001B[0m \u001B[38;5;124;03m        OptionalImportError: When you call this method.\u001B[39;00m\n\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 443\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n",
      "File \u001B[0;32m~/.venv/AML_Project_Supervised/lib/python3.10/site-packages/monai/utils/module.py:395\u001B[0m, in \u001B[0;36moptional_import\u001B[0;34m(module, version, version_checker, name, descriptor, version_args, allow_namespace_pkg, as_type)\u001B[0m\n\u001B[1;32m    393\u001B[0m     actual_cmd \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimport \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 395\u001B[0m     pkg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43m__import__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# top level module\u001B[39;00m\n\u001B[1;32m    396\u001B[0m     the_module \u001B[38;5;241m=\u001B[39m import_module(module)\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_namespace_pkg:\n",
      "\u001B[0;31mOptionalImportError\u001B[0m: from einops.layers.torch import Rearrange (No module named 'einops').\n\nFor details about installing the optional dependencies, please visit:\n    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "60b1912d",
   "metadata": {},
   "source": [
    "##### Training loop with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d71ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_cl_loss = 0\n",
    "    epoch_recon_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        inputs, inputs_2, gt_input = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"image_2\"].to(device),\n",
    "            batch_data[\"gt_image\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs_v1, hidden_v1 = model(inputs)\n",
    "        outputs_v2, hidden_v2 = model(inputs_2)\n",
    "\n",
    "        flat_out_v1 = outputs_v1.flatten(start_dim=1, end_dim=4)\n",
    "        flat_out_v2 = outputs_v2.flatten(start_dim=1, end_dim=4)\n",
    "\n",
    "        r_loss = recon_loss(outputs_v1, gt_input)\n",
    "        cl_loss = contrastive_loss(flat_out_v1, flat_out_v2)\n",
    "\n",
    "        # Adjust the CL loss by Recon Loss\n",
    "        total_loss = r_loss + cl_loss * r_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += total_loss.item()\n",
    "        step_loss_values.append(total_loss.item())\n",
    "\n",
    "        # CL & Recon Loss Storage of Value\n",
    "        epoch_cl_loss += cl_loss.item()\n",
    "        epoch_recon_loss += r_loss.item()\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "            f\"train_loss: {total_loss.item():.4f}, \"\n",
    "            f\"time taken: {end_time-start_time}s\"\n",
    "        )\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_cl_loss /= step\n",
    "    epoch_recon_loss /= step\n",
    "\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    epoch_cl_loss_values.append(epoch_cl_loss)\n",
    "    epoch_recon_loss_values.append(epoch_recon_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if epoch % val_interval == 0:\n",
    "        print(\"Entering Validation for epoch: {}\".format(epoch + 1))\n",
    "        total_val_loss = 0\n",
    "        val_step = 0\n",
    "        model.eval()\n",
    "        for val_batch in val_loader:\n",
    "            val_step += 1\n",
    "            start_time = time.time()\n",
    "            inputs, gt_input = (\n",
    "                val_batch[\"image\"].to(device),\n",
    "                val_batch[\"gt_image\"].to(device),\n",
    "            )\n",
    "            print(\"Input shape: {}\".format(inputs.shape))\n",
    "            outputs, outputs_v2 = model(inputs)\n",
    "            val_loss = recon_loss(outputs, gt_input)\n",
    "            total_val_loss += val_loss.item()\n",
    "            end_time = time.time()\n",
    "\n",
    "        total_val_loss /= val_step\n",
    "        val_loss_values.append(total_val_loss)\n",
    "        print(f\"epoch {epoch + 1} Validation avg loss: {total_val_loss:.4f}, \" f\"time taken: {end_time-start_time}s\")\n",
    "\n",
    "        if total_val_loss < best_val_loss:\n",
    "            print(f\"Saving new model based on validation loss {total_val_loss:.4f}\")\n",
    "            best_val_loss = total_val_loss\n",
    "            checkpoint = {\"epoch\": max_epochs, \"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "            torch.save(checkpoint, os.path.join(logdir_path, \"best_model.pt\"))\n",
    "\n",
    "        plt.figure(1, figsize=(8, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(epoch_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(val_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Validation Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(epoch_cl_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Contrastive Loss\")\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(epoch_recon_loss_values)\n",
    "        plt.grid()\n",
    "        plt.title(\"Training Recon Loss\")\n",
    "\n",
    "        plt.savefig(os.path.join(logdir_path, \"loss_plots.png\"))\n",
    "        plt.close(1)\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "da3e08083059755bb70e9f8b58ba677201225f59652efa5b6b39528ae9381865"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
